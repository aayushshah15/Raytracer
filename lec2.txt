Date: 5th January 2017

Matrix Multiplication
It is "easy" to see that there are at least n^2 operations to be done as a lower bound
It is clear that one upper bound is n^3 (since we do n operations to compute each of the n^2 elements in the result matrix)

In 1969 it was proven that we can get a running time of O(n^2.78034), and in 1985 it was proven that we can get a running time of O(n^2.304872), which is the best known yet.

Now, proof from the last lecture:
What is a limit?
for all eps > 0, there exists an n_0 such that |f(n)/g(n) - L| <= eps for all n >= n_0
That is the definition of a limit

Lets pick eps == 1
then there exists an n_0 such that f(n)/g(n) - L <= |f(n)/g(n) - L| <= 1
^ this is true for all n >= n_0
we know that g(n) > 0, 

f(n) - L.g(n) <= g(n)
f(n) <= (L+1).g(n) for all n >= n_0

now lets say (L+1) == c
(notice that c >= 0)
By definition: f(n) \in O(g(n)). 
prove it similarly for the omega


Corollary 1:
Polynomial rule
Let f(n) be a degree d polynomial, then f(n) is theta(n^d)
Proof:
Suppose f(n) = a_dn^d + a_(d-1)n^(d-1).......a_1n + a_0
and a_d > 0 (because running times are always positive)
I only care if the first one has a positive coefficient

Then use the limit rule,
lim as n->inf f(n)/n^d = lim as n->inf a_dn^d/n^d .....lim as n->inf a_0/n^d = a_d
Thus, f(n) \in theta(n^d)


Corollary 2: Logarithms grow much smaller than linear
For arbitrarily small i > 0, we have log_n \in o(n^i)
For arbitrarily large k > 0, we have log^k(n) \in o(n)

Proof:
Use the limit lemma,
lim as n->inf log(n)/n^i = (use hospitals rule)
lim as n->inf (ln(n)/ln(2))/n^i = lim as n->inf (1/nln2)/i.n^i-1
... incomplete ...

Lemma 2:
If f and g are positive functions with f(n) \in o(g(n)) then the limit as n->inf f(n)/g(n) exists and is 0

Proof:
Pick an arbitrary eps > 0.
Must show:
there exists n_0 > 0 such that |f(n)/g(n)| <= eps for all n >= n_0
But we know that f(n) \in o(g(n)) 
So we know that for any c > 0, there exists n_0 such that f(n) < c.g(n) for all n >= n_0
Notice that g(n) > 0: f(n)/g(n) <= eps for all n >= n_0. Notice f(n) > 0

Typical exam question:

f(n)  ---- given 
g(n) ----- given
is f(n) \in ___ g(n)

f(n)  |  g(n)  |  w  | omega  | O  | o
log n   sqrt(n)   no    no     yes  yes
100n^3  n^2      yes   yes      no  no
100n^3  n^3      no    yes     yes  no

Relationships:
If f(n) \in o(g(n)) ---> f(n) \in O(g(n))
if f(n) \in w(g(n)) ----> f(n) \in omega(g(n))
if f(n) \in omega(g(n)) -----> f(n) not \in o(g(n))
  proof:
  then there exists some c > 0 and n_0 > 0 such that f(n) > c.g(n) for all n >= n_0
  then lim as n->inf f(n)/g(n) either doesn't exist or >= c
  But we know that for f(n) to be \in g(n) that ^ limit must be 0.
  but c > 0 and lim as n->inf f(n)/g(n) >= c, its clear that it cant be 0
  thus f(n) not \in g(n)
if f(n) \in O(g(n)) ----> f(n) not \in w(g(n))
contrapositives:............
.......
for f, g, at most two of o, w, O, omega hold.
fun exercise: find f and g where only 1 holds.


Algorithm design: (for example mergesort)
1) Describe the algorithm at a high level in english "break in half, sort both halves and merge together"
2) Pseudocode
3) Argue correctness 
  (Use induction for mergesort)
  Talk about it working for smaller cases and then talk about the bigger cases being divided into those smaller cases and being merged
  Prove merge is correct
  Make an argument about what C (from the slides) contains.
4) Analyse the runtime
  MergeSort has running time is 
  T(n) = {
    O(1) if n <= 1
    2T(n/2) + O(n)
  }